{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-1.6.0-cp36-cp36m-manylinux1_x86_64.whl (45.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 45.9MB 30kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting gast>=0.2.0 (from tensorflow)\n",
      "  Downloading gast-0.2.0.tar.gz\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-1.1.0.tar.gz\n",
      "Collecting absl-py>=0.1.6 (from tensorflow)\n",
      "  Downloading absl-py-0.1.13.tar.gz (80kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 11.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting grpcio>=1.8.6 (from tensorflow)\n",
      "  Downloading grpcio-1.10.0-cp36-cp36m-manylinux1_x86_64.whl (7.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 7.5MB 187kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<1.7.0,>=1.6.0 (from tensorflow)\n",
      "  Downloading tensorboard-1.6.0-py3-none-any.whl (3.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.1MB 465kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.4.0 in /home/ubuntu/anaconda3/lib/python3.6/site-packages (from tensorflow)\n",
      "Collecting astor>=0.6.0 (from tensorflow)\n",
      "  Downloading astor-0.6.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: wheel>=0.26 in /home/ubuntu/anaconda3/lib/python3.6/site-packages (from tensorflow)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/ubuntu/anaconda3/lib/python3.6/site-packages (from tensorflow)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ubuntu/anaconda3/lib/python3.6/site-packages (from tensorflow)\n",
      "Collecting bleach==1.5.0 (from tensorboard<1.7.0,>=1.6.0->tensorflow)\n",
      "  Downloading bleach-1.5.0-py2.py3-none-any.whl\n",
      "Collecting html5lib==0.9999999 (from tensorboard<1.7.0,>=1.6.0->tensorflow)\n",
      "  Downloading html5lib-0.9999999.tar.gz (889kB)\n",
      "\u001b[K    100% |████████████████████████████████| 890kB 1.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.10 in /home/ubuntu/anaconda3/lib/python3.6/site-packages (from tensorboard<1.7.0,>=1.6.0->tensorflow)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.7.0,>=1.6.0->tensorflow)\n",
      "  Downloading Markdown-2.6.11-py2.py3-none-any.whl (78kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 11.7MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/ubuntu/anaconda3/lib/python3.6/site-packages (from protobuf>=3.4.0->tensorflow)\n",
      "Building wheels for collected packages: gast, termcolor, absl-py, html5lib\n",
      "  Running setup.py bdist_wheel for gast ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ubuntu/.cache/pip/wheels/8e/fa/d6/77dd17d18ea23fd7b860e02623d27c1be451521af40dd4a13e\n",
      "  Running setup.py bdist_wheel for termcolor ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ubuntu/.cache/pip/wheels/de/f7/bf/1bcac7bf30549e6a4957382e2ecab04c88e513117207067b03\n",
      "  Running setup.py bdist_wheel for absl-py ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ubuntu/.cache/pip/wheels/76/f7/0c/88796d7212af59bb2f496b12267e0605f205170781e9b86479\n",
      "  Running setup.py bdist_wheel for html5lib ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ubuntu/.cache/pip/wheels/6f/85/6c/56b8e1292c6214c4eb73b9dda50f53e8e977bf65989373c962\n",
      "Successfully built gast termcolor absl-py html5lib\n",
      "Installing collected packages: gast, termcolor, absl-py, grpcio, html5lib, bleach, markdown, tensorboard, astor, tensorflow\n",
      "  Found existing installation: html5lib 1.0.1\n",
      "    Uninstalling html5lib-1.0.1:\n",
      "      Successfully uninstalled html5lib-1.0.1\n",
      "  Found existing installation: bleach 2.1.2\n",
      "    Uninstalling bleach-2.1.2:\n",
      "      Successfully uninstalled bleach-2.1.2\n",
      "Successfully installed absl-py-0.1.13 astor-0.6.2 bleach-1.5.0 gast-0.2.0 grpcio-1.10.0 html5lib-0.9999999 markdown-2.6.11 tensorboard-1.6.0 tensorflow-1.6.0 termcolor-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading Keras-2.1.5-py2.py3-none-any.whl (334kB)\n",
      "\u001b[K    100% |████████████████████████████████| 337kB 3.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.14 in /home/ubuntu/anaconda3/lib/python3.6/site-packages (from keras)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/ubuntu/anaconda3/lib/python3.6/site-packages (from keras)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/ubuntu/anaconda3/lib/python3.6/site-packages (from keras)\n",
      "Requirement already satisfied: pyyaml in /home/ubuntu/anaconda3/lib/python3.6/site-packages (from keras)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.1.5\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from lightkurve import KeplerTargetPixelFile\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from sklearn import metrics\n",
    "from keras.utils import to_categorical\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load in x and y vectors from files\n",
    "- y must be converted to categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('xtrainfile2.npy').reshape(5725, 1, 1626).astype('float32')\n",
    "X_test = np.load('xtestfile2.npy').reshape(2160, 1, 1626).astype('float32')\n",
    "Y_train = np.load('ytrainfile2.npy')\n",
    "Y_test = np.load('ytestfile2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y needs to be categorical\n",
    "y_trainbinary = to_categorical(Y_train)\n",
    "y_testbinary = to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find the shapes of each vector, as confirmation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (5725, 1, 1626)\n",
      "y_trainbinary: (5725, 2)\n",
      "----\n",
      "X_test: (2160, 1, 1626)\n",
      "y_testbinary: (2160, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train: \"+str(X_train.shape))\n",
    "print(\"y_trainbinary: \"+str(y_trainbinary.shape))\n",
    "print(\"----\")\n",
    "print(\"X_test: \"+str(X_test.shape))\n",
    "print(\"y_testbinary: \"+str(y_testbinary.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.99901456, 0.99931747, 0.9984102 , ..., 0.9997821 ,\n",
       "         1.0010506 , 1.0008373 ]],\n",
       "\n",
       "       [[0.99901456, 0.99931747, 0.9984102 , ..., 0.9997821 ,\n",
       "         1.0010506 , 1.0008373 ]],\n",
       "\n",
       "       [[1.0051923 , 1.0036092 , 1.0033753 , ..., 0.9927544 ,\n",
       "         0.9935881 , 0.99231523]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.9997914 , 1.0000747 , 0.9999657 , ..., 1.0004965 ,\n",
       "         0.9999225 , 0.99977523]],\n",
       "\n",
       "       [[1.0004292 , 0.9997566 , 1.001402  , ..., 1.0006493 ,\n",
       "         1.0006902 , 0.99959934]],\n",
       "\n",
       "       [[1.0000124 , 0.9999862 , 1.0001131 , ..., 0.9997616 ,\n",
       "         0.99982774, 1.0001394 ]]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialize variables and placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, 1626, 1))\n",
    "\n",
    "w1 = tf.Variable(tf.zeros([1626, 1000]))\n",
    "w2 = tf.Variable(tf.zeros([1000, 100]))\n",
    "w3 = tf.Variable(tf.zeros([100, 10]))\n",
    "w4 = tf.Variable(tf.zeros([10, 2]))\n",
    "\n",
    "b1 = tf.Variable(tf.zeros([1000]))\n",
    "b2 = tf.Variable(tf.zeros([100]))\n",
    "b3 = tf.Variable(tf.zeros([10]))\n",
    "b4 = tf.Variable(tf.zeros([2]))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "# not sure if I need this, but just in case\n",
    "X = tf.reshape(X, [-1, 1626])\n",
    "\n",
    "Y1 = tf.nn.sigmoid(tf.matmul(X, w1) + b1)\n",
    "Y2 = tf.nn.sigmoid(tf.matmul(Y1, w2) + b2)\n",
    "Y3 = tf.nn.sigmoid(tf.matmul(Y2, w3) + b3)\n",
    "Ypredicted = tf.nn.softmax(tf.matmul(Y3, w4) + b4)\n",
    "\n",
    "\n",
    "Y_ = tf.placeholder(tf.float32, shape=(None, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define optimizers and learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = -(tf.reduce_sum(Y_*tf.log(Ypredicted)))\n",
    "is_correct = tf.equal(tf.argmax(Ypredicted, 1), tf.argmax(Y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.003)\n",
    "train_step = optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Start the Tensorflow Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test and train simultaneously with 60 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 3/60 [00:00<00:05,  9.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 5/60 [00:00<00:05, 10.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 9/60 [00:00<00:04, 10.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|█▊        | 11/60 [00:00<00:04, 11.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 15/60 [00:01<00:04, 11.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██▊       | 17/60 [00:01<00:03, 11.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 21/60 [00:01<00:03, 11.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███▊      | 23/60 [00:02<00:03, 11.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 27/60 [00:02<00:02, 11.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|████▊     | 29/60 [00:02<00:02, 11.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 33/60 [00:02<00:02, 11.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|█████▊    | 35/60 [00:03<00:02, 11.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 39/60 [00:03<00:01, 11.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|██████▊   | 41/60 [00:03<00:01, 11.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 45/60 [00:03<00:01, 11.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|███████▊  | 47/60 [00:04<00:01, 11.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 51/60 [00:04<00:00, 11.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 53/60 [00:04<00:00, 11.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 57/60 [00:04<00:00, 11.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:05<00:00, 11.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n",
      "Tensor(\"Equal_18:0\", shape=(?,), dtype=bool)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(60)):\n",
    "    train_data = {X:X_train, Y_:y_train}\n",
    "    sess.run(train_step, feed_dict=train_data)\n",
    "    a,c = sess.run([accuracy, cross_entropy], feed_dict=train_data)\n",
    "    print(is_correct)\n",
    "    test_data = {X:X_test, Y_:y_test}\n",
    "    a,c = sess.run([accuracy, cross_entropy], feed_dict=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define function that returns error and confusions\n",
    "- This will be useful for creating the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "def error_rate(predictions, labels):\n",
    "    \"\"\"Return the error rate and confusions.\"\"\"\n",
    "    is_correct = tf.equal(tf.argmax(Ypredicted, 1), tf.argmax(Y_,1))\n",
    "    correct = np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "    total = predictions.shape[0]\n",
    "\n",
    "    error = 100.0 - (100 * float(correct) / float(total))\n",
    "\n",
    "    confusions = np.zeros([10, 10], np.float32)\n",
    "    bundled = zip(np.argmax(predictions, 1), np.argmax(labels, 1))\n",
    "    for predicted, actual in bundled:\n",
    "        confusions[predicted, actual] += 1\n",
    "    \n",
    "    return error, confusions\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Print Confusion Matrix and Test Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error: 0.0%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd13e2b6048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_error, confusions = error_rate(Ypredicted.eval(session=sess, feed_dict=test_data), y_test)\n",
    "print('Test error: %.1f%%' % test_error)\n",
    "\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.grid(True)\n",
    "plt.xticks(np.arange(1,3, dtype=int))\n",
    "plt.yticks(np.arange(1,3, 1, dtype=int))\n",
    "plt.imshow(confusions); #cmap=plt.cm.jet, interpolation='nearest');\n",
    "\n",
    "#for i, cas in enumerate(confusions):\n",
    " #   for j, count in enumerate(cas):\n",
    "  #      if count > 0:\n",
    "  #          xoff = .07 * len(str(count))\n",
    "   #         plt.text(j-xoff, i+.2, int(count), fontsize=9, color='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                         normalize=False,\n",
    "                         title='Confusion matrix',\n",
    "                         cmap=plt.cm.Reds):\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, cmap=cmap)#interpolation='nearest'\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > thresh else \"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 100.00%\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of unknown and binary targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-151-a9ea01fbc56c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtrain_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYpredicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model accuracy: {:.2f}%\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mYpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'absent'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'present'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \"\"\"\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 81\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of unknown and binary targets"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd13e3c3198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.tight_layout()\n",
    "plt.title(\"KNN Confusion Matrix\", fontweight=\"bold\")\n",
    "plt.ylabel('True label', fontweight='bold')\n",
    "plt.xlabel('Predicted label', fontweight='bold')\n",
    "\n",
    "train_error, confus = error_rate(Ypredicted.eval(session=sess, feed_dict=train_data), y_test)\n",
    "print(\"Model accuracy: {:.2f}%\".format(train_error))\n",
    "plot_confusion_matrix(metrics.confusion_matrix((y_test,Ypredicted), ['absent', 'present']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# background investigating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tempfile\n",
    "import keras\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(123)  # for reproducibility\n",
    " \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test for correct backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "if K.backend() != 'tensorflow':\n",
    "    raise RuntimeError('This example can only run with the TensorFlow backend,'\n",
    "                       ' because it requires the Datset API, which is not'\n",
    "                       ' supported on other platforms.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Construct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the keras tutorial:\n",
    "model2 = Sequential()\n",
    "\n",
    "model2.add(Dense(2, input_shape=(1626, 1)))\n",
    "\n",
    "model2.add(Flatten())\n",
    "\n",
    "model2.add(Dense(3000, activation='sigmoid'))\n",
    "\n",
    "#model.add(Dropout(0.25))\n",
    "\n",
    "model2.add(Dense(300, activation='sigmoid'))\n",
    "model2.add(Dense(30, activation='sigmoid'))\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "model2.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compile model, define loss, optimizer, and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# still from the keras tutorial\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "* Show model architecture and outputs for each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_26 (Dense)             (None, 1626, 2)           4         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 3252)              0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 3000)              9759000   \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 300)               900300    \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 30)                9030      \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 2)                 62        \n",
      "=================================================================\n",
      "Total params: 10,668,396\n",
      "Trainable params: 10,668,396\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fit model to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "5725/5725 [==============================] - 44s 8ms/step - loss: 0.6830 - acc: 0.5843\n",
      "Epoch 2/16\n",
      "5725/5725 [==============================] - 44s 8ms/step - loss: 0.6805 - acc: 0.5857\n",
      "Epoch 3/16\n",
      "5725/5725 [==============================] - 44s 8ms/step - loss: 0.6800 - acc: 0.5857\n",
      "Epoch 4/16\n",
      "5725/5725 [==============================] - 43s 8ms/step - loss: 0.6787 - acc: 0.5857\n",
      "Epoch 5/16\n",
      "5725/5725 [==============================] - 43s 8ms/step - loss: 0.6796 - acc: 0.5857\n",
      "Epoch 6/16\n",
      "5725/5725 [==============================] - 44s 8ms/step - loss: 0.6794 - acc: 0.5857\n",
      "Epoch 7/16\n",
      "5725/5725 [==============================] - 44s 8ms/step - loss: 0.6802 - acc: 0.5857\n",
      "Epoch 8/16\n",
      "5725/5725 [==============================] - 43s 8ms/step - loss: 0.6792 - acc: 0.5857\n",
      "Epoch 9/16\n",
      "5725/5725 [==============================] - 43s 8ms/step - loss: 0.6799 - acc: 0.5857\n",
      "Epoch 10/16\n",
      "5725/5725 [==============================] - 43s 8ms/step - loss: 0.6802 - acc: 0.5857\n",
      "Epoch 11/16\n",
      "5725/5725 [==============================] - 44s 8ms/step - loss: 0.6789 - acc: 0.5857\n",
      "Epoch 12/16\n",
      "5725/5725 [==============================] - 43s 8ms/step - loss: 0.6793 - acc: 0.5857\n",
      "Epoch 13/16\n",
      "5725/5725 [==============================] - 43s 8ms/step - loss: 0.6797 - acc: 0.5857\n",
      "Epoch 14/16\n",
      "5725/5725 [==============================] - 43s 8ms/step - loss: 0.6801 - acc: 0.5857\n",
      "Epoch 15/16\n",
      "5725/5725 [==============================] - 44s 8ms/step - loss: 0.6793 - acc: 0.5857\n",
      "Epoch 16/16\n",
      "5725/5725 [==============================] - 43s 8ms/step - loss: 0.6789 - acc: 0.5857\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3baa49e438>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# throws an error, out of memory\n",
    "model2.fit(X_train, y_trainbinary, \n",
    "          batch_size=16, nb_epoch=16, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluate model based on accuracy with the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2160/2160 [==============================] - 1s 540us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.81629329389996, 0.24305555555555555]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(X_test, y_testbinary, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Model 2 \n",
    "## (includes decaying learning rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tempfile\n",
    "import keras\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(123)  # for reproducibility\n",
    " \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test for correct backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if K.backend() != 'tensorflow':\n",
    "    raise RuntimeError('This example can only run with the TensorFlow backend,'\n",
    "                       ' because it requires the Datset API, which is not'\n",
    "                       ' supported on other platforms.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Construct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the keras tutorial:\n",
    "model3 = Sequential()\n",
    "\n",
    "model3.add(Dense(2, input_shape=(1, 1626)))\n",
    "\n",
    "model3.add(Flatten())\n",
    "\n",
    "model3.add(Dense(3000, activation='sigmoid'))\n",
    "\n",
    "#model.add(Dropout(0.25))\n",
    "\n",
    "model3.add(Dense(300, activation='sigmoid'))\n",
    "model3.add(Dense(30, activation='sigmoid'))\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "model3.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Visualization: shows each layer in the model, and its output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 1, 2)              3254      \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 3000)              9000      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 300)               900300    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 30)                9030      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 2)                 62        \n",
      "=================================================================\n",
      "Total params: 921,646\n",
      "Trainable params: 921,646\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## from the online resource (to get a decaying learning rate)\n",
    "\n",
    "# Compile model\n",
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "decay_rate = learning_rate / epochs\n",
    "momentum = 0.8\n",
    "sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "model3.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5725 samples, validate on 2160 samples\n",
      "Epoch 1/10\n",
      "5725/5725 [==============================] - 2s 343us/step - loss: 0.6845 - acc: 0.5710 - val_loss: 0.9115 - val_acc: 0.2431\n",
      "Epoch 2/10\n",
      "5725/5725 [==============================] - 2s 291us/step - loss: 0.6821 - acc: 0.5797 - val_loss: 0.7121 - val_acc: 0.2431\n",
      "Epoch 3/10\n",
      "5725/5725 [==============================] - 2s 295us/step - loss: 0.6819 - acc: 0.5857 - val_loss: 0.7965 - val_acc: 0.2431\n",
      "Epoch 4/10\n",
      "5725/5725 [==============================] - 2s 292us/step - loss: 0.6813 - acc: 0.5857 - val_loss: 0.7317 - val_acc: 0.2431\n",
      "Epoch 5/10\n",
      "5725/5725 [==============================] - 2s 292us/step - loss: 0.6811 - acc: 0.5857 - val_loss: 0.8166 - val_acc: 0.2431\n",
      "Epoch 6/10\n",
      "5725/5725 [==============================] - 2s 290us/step - loss: 0.6806 - acc: 0.5857 - val_loss: 0.7828 - val_acc: 0.2431\n",
      "Epoch 7/10\n",
      "5725/5725 [==============================] - 2s 296us/step - loss: 0.6805 - acc: 0.5857 - val_loss: 0.7858 - val_acc: 0.2431\n",
      "Epoch 8/10\n",
      "5725/5725 [==============================] - 2s 296us/step - loss: 0.6800 - acc: 0.5857 - val_loss: 0.7941 - val_acc: 0.2431\n",
      "Epoch 9/10\n",
      "5725/5725 [==============================] - 2s 297us/step - loss: 0.6810 - acc: 0.5857 - val_loss: 0.8581 - val_acc: 0.2431\n",
      "Epoch 10/10\n",
      "5725/5725 [==============================] - 2s 294us/step - loss: 0.6810 - acc: 0.5857 - val_loss: 0.8248 - val_acc: 0.2431\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2d298fcbe0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model3.fit(X_train, y_trainbinary, epochs=epochs, batch_size=28, validation_data=(X_test, y_testbinary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluate the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2160/2160 [==============================] - 1s 536us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7850716997075964, 0.24305555555555555]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.evaluate(X_test, y_testbinary, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_layers(inputs):\n",
    "    x = layers.Conv2D(32, (3, 3),\n",
    "                      activation='relu', padding='valid')(inputs)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    predictions = layers.Dense(classes,\n",
    "                               activation='softmax',\n",
    "                               name='x_train_out')(x)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "buffer_size = 10000\n",
    "steps_per_epoch = np.ceil(80000 / batch_size).astype('int')  # = 469\n",
    "epochs = 5\n",
    "classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('xtrainfile.npy')\n",
    "X_test = np.load('xtestfile.npy')\n",
    "Y_train = np.load('ytrainfile.npy')\n",
    "Y_test = np.load('ytestfile.npy')\n",
    "\n",
    "X_train = x_train.astype(np.float32)\n",
    "X_train = np.expand_dims(x_train, -1)\n",
    "Y_train = Y_train.astype(np.int32)\n",
    "Y_train = tf.one_hot(Y_train, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer conv2d_3: expected ndim=4, found ndim=16",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-188-5540d7a217d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mtrain_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-167-aa2b4d99bccf>\u001b[0m in \u001b[0;36mcnn_layers\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcnn_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     x = layers.Conv2D(32, (3, 3),\n\u001b[0;32m----> 3\u001b[0;31m                       activation='relu', padding='valid')(inputs)\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    470\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m                                      str(K.ndim(x)))\n\u001b[0m\u001b[1;32m    473\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 is incompatible with layer conv2d_3: expected ndim=4, found ndim=16"
     ]
    }
   ],
   "source": [
    "# Create the dataset and its associated one-shot iterator.\n",
    "dataset = Dataset.from_tensor_slices((x_train, y_train))\n",
    "dataset = dataset.repeat()\n",
    "dataset = dataset.shuffle(buffer_size)\n",
    "dataset = dataset.batch(batch_size)\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "# Model creation using tensors from the get_next() graph node.\n",
    "inputs, targets = iterator.get_next()\n",
    "model_input = layers.Input(tensor=inputs)\n",
    "model_output = cnn_layers(model_input)\n",
    "train_model = keras.models.Model(inputs=model_input, outputs=model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               819712    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "x_train_out (Dense)          (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 843,658\n",
      "Trainable params: 843,658\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "train_model.compile(optimizer=keras.optimizers.RMSprop(lr=2e-3, decay=1e-5),\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['accuracy'],\n",
    "                    target_tensors=[targets])\n",
    "train_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 10s 21ms/step - loss: 0.1626 - acc: 0.9495\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0519 - acc: 0.9844\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0359 - acc: 0.9886\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0292 - acc: 0.9911\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 5s 11ms/step - loss: 0.0255 - acc: 0.9925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd1629131d0>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model.fit(epochs=epochs,\n",
    "                steps_per_epoch=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model weights.\n",
    "weight_path = os.path.join(tempfile.gettempdir(), 'saved_wt.h5')\n",
    "train_model.save_weights(weight_path)\n",
    "\n",
    "# Clean up the TF session.\n",
    "K.clear_session()\n",
    "\n",
    "# Second session to test loading trained model without tensors.\n",
    "x_test = x_test.astype(np.float32)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "x_test_inp = layers.Input(shape=x_test.shape[1:])\n",
    "test_out = cnn_layers(x_test_inp)\n",
    "test_model = keras.models.Model(inputs=x_test_inp, outputs=test_out)\n",
    "\n",
    "test_model.load_weights(weight_path)\n",
    "test_model.compile(optimizer='rmsprop',\n",
    "                   loss='sparse_categorical_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "test_model.summary()\n",
    "\n",
    "loss, acc = test_model.evaluate(x_test, y_test, classes)\n",
    "print('\\nTest accuracy: {0}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random forest with percentiles\n",
    "min value\n",
    "max value\n",
    "first percentile\n",
    "99th percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
